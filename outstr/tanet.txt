Recognizer2D(
  24.781 M, 100.000% Params, 65.94 GFLOPs, 100.000% FLOPs, 
  (backbone): TANet(
    24.769 M, 99.950% Params, 65.938 GFLOPs, 99.997% FLOPs, 
    (conv1): ConvModule(
      0.01 M, 0.038% Params, 1.927 GFLOPs, 2.922% FLOPs, 
      (conv): Conv2d(0.009 M, 0.038% Params, 1.888 GFLOPs, 2.864% FLOPs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.026 GFLOPs, 0.039% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activate): ReLU(0.0 M, 0.000% Params, 0.013 GFLOPs, 0.019% FLOPs, inplace=True)
    )
    (maxpool): MaxPool2d(0.0 M, 0.000% Params, 0.013 GFLOPs, 0.019% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      0.228 M, 0.922% Params, 10.886 GFLOPs, 16.510% FLOPs, 
      (0): TABlock(
        0.079 M, 0.320% Params, 3.783 GFLOPs, 5.737% FLOPs, 
        (block): Bottleneck(
          0.075 M, 0.303% Params, 3.783 GFLOPs, 5.737% FLOPs, 
          (conv1): ConvModule(
            0.004 M, 0.017% Params, 0.215 GFLOPs, 0.326% FLOPs, 
            (conv): Conv2d(0.004 M, 0.017% Params, 0.206 GFLOPs, 0.312% FLOPs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.006 GFLOPs, 0.010% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.003 GFLOPs, 0.005% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            0.037 M, 0.149% Params, 1.859 GFLOPs, 2.820% FLOPs, 
            (conv): Conv2d(0.037 M, 0.149% Params, 1.85 GFLOPs, 2.805% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.006 GFLOPs, 0.010% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.003 GFLOPs, 0.005% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            0.017 M, 0.068% Params, 0.848 GFLOPs, 1.286% FLOPs, 
            (conv): Conv2d(0.016 M, 0.066% Params, 0.822 GFLOPs, 1.247% FLOPs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.026 GFLOPs, 0.039% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.013 GFLOPs, 0.019% FLOPs, inplace=True)
          (downsample): ConvModule(
            0.017 M, 0.068% Params, 0.848 GFLOPs, 1.286% FLOPs, 
            (conv): Conv2d(0.016 M, 0.066% Params, 0.822 GFLOPs, 1.247% FLOPs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.026 GFLOPs, 0.039% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (tam): TAM(
          0.004 M, 0.017% Params, 0.0 GFLOPs, 0.000% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.004 M, 0.017% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Conv1d(0.003 M, 0.012% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.001 M, 0.004% Params, 0.0 GFLOPs, 0.000% FLOPs, 16, 64, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
      (1): TABlock(
        0.075 M, 0.301% Params, 3.552 GFLOPs, 5.386% FLOPs, 
        (block): Bottleneck(
          0.07 M, 0.284% Params, 3.552 GFLOPs, 5.386% FLOPs, 
          (conv1): ConvModule(
            0.017 M, 0.067% Params, 0.832 GFLOPs, 1.261% FLOPs, 
            (conv): Conv2d(0.016 M, 0.066% Params, 0.822 GFLOPs, 1.247% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.006 GFLOPs, 0.010% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.003 GFLOPs, 0.005% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            0.037 M, 0.149% Params, 1.859 GFLOPs, 2.820% FLOPs, 
            (conv): Conv2d(0.037 M, 0.149% Params, 1.85 GFLOPs, 2.805% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.006 GFLOPs, 0.010% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.003 GFLOPs, 0.005% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            0.017 M, 0.068% Params, 0.848 GFLOPs, 1.286% FLOPs, 
            (conv): Conv2d(0.016 M, 0.066% Params, 0.822 GFLOPs, 1.247% FLOPs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.026 GFLOPs, 0.039% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.013 GFLOPs, 0.019% FLOPs, inplace=True)
        )
        (tam): TAM(
          0.004 M, 0.017% Params, 0.0 GFLOPs, 0.000% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.004 M, 0.017% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Conv1d(0.003 M, 0.012% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.001 M, 0.004% Params, 0.0 GFLOPs, 0.000% FLOPs, 16, 64, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
      (2): TABlock(
        0.075 M, 0.301% Params, 3.552 GFLOPs, 5.386% FLOPs, 
        (block): Bottleneck(
          0.07 M, 0.284% Params, 3.552 GFLOPs, 5.386% FLOPs, 
          (conv1): ConvModule(
            0.017 M, 0.067% Params, 0.832 GFLOPs, 1.261% FLOPs, 
            (conv): Conv2d(0.016 M, 0.066% Params, 0.822 GFLOPs, 1.247% FLOPs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.006 GFLOPs, 0.010% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.003 GFLOPs, 0.005% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            0.037 M, 0.149% Params, 1.859 GFLOPs, 2.820% FLOPs, 
            (conv): Conv2d(0.037 M, 0.149% Params, 1.85 GFLOPs, 2.805% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.006 GFLOPs, 0.010% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.003 GFLOPs, 0.005% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            0.017 M, 0.068% Params, 0.848 GFLOPs, 1.286% FLOPs, 
            (conv): Conv2d(0.016 M, 0.066% Params, 0.822 GFLOPs, 1.247% FLOPs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.026 GFLOPs, 0.039% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.013 GFLOPs, 0.019% FLOPs, inplace=True)
        )
        (tam): TAM(
          0.004 M, 0.017% Params, 0.0 GFLOPs, 0.000% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.004 M, 0.017% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Conv1d(0.003 M, 0.012% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.001 M, 0.004% Params, 0.0 GFLOPs, 0.000% FLOPs, 16, 64, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
    )
    (layer2): Sequential(
      1.286 M, 5.188% Params, 16.586 GFLOPs, 25.153% FLOPs, 
      (0): TABlock(
        0.396 M, 1.598% Params, 6.017 GFLOPs, 9.124% FLOPs, 
        (block): Bottleneck(
          0.379 M, 1.531% Params, 6.016 GFLOPs, 9.124% FLOPs, 
          (conv1): ConvModule(
            0.033 M, 0.133% Params, 1.663 GFLOPs, 2.523% FLOPs, 
            (conv): Conv2d(0.033 M, 0.132% Params, 1.644 GFLOPs, 2.493% FLOPs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.013 GFLOPs, 0.019% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.006 GFLOPs, 0.010% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            0.148 M, 0.596% Params, 1.855 GFLOPs, 2.812% FLOPs, 
            (conv): Conv2d(0.147 M, 0.595% Params, 1.85 GFLOPs, 2.805% FLOPs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.003 GFLOPs, 0.005% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.002 GFLOPs, 0.002% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            0.067 M, 0.269% Params, 0.835 GFLOPs, 1.266% FLOPs, 
            (conv): Conv2d(0.066 M, 0.264% Params, 0.822 GFLOPs, 1.247% FLOPs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.004% Params, 0.013 GFLOPs, 0.019% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.006 GFLOPs, 0.010% FLOPs, inplace=True)
          (downsample): ConvModule(
            0.132 M, 0.533% Params, 1.657 GFLOPs, 2.513% FLOPs, 
            (conv): Conv2d(0.131 M, 0.529% Params, 1.644 GFLOPs, 2.493% FLOPs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.004% Params, 0.013 GFLOPs, 0.019% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (tam): TAM(
          0.017 M, 0.067% Params, 0.0 GFLOPs, 0.000% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.016 M, 0.066% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Conv1d(0.012 M, 0.050% Params, 0.0 GFLOPs, 0.000% FLOPs, 128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.004 M, 0.017% Params, 0.0 GFLOPs, 0.000% FLOPs, 32, 128, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
      (1): TABlock(
        0.297 M, 1.197% Params, 3.523 GFLOPs, 5.343% FLOPs, 
        (block): Bottleneck(
          0.28 M, 1.130% Params, 3.523 GFLOPs, 5.342% FLOPs, 
          (conv1): ConvModule(
            0.066 M, 0.265% Params, 0.827 GFLOPs, 1.254% FLOPs, 
            (conv): Conv2d(0.066 M, 0.264% Params, 0.822 GFLOPs, 1.247% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.003 GFLOPs, 0.005% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.002 GFLOPs, 0.002% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            0.148 M, 0.596% Params, 1.855 GFLOPs, 2.812% FLOPs, 
            (conv): Conv2d(0.147 M, 0.595% Params, 1.85 GFLOPs, 2.805% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.003 GFLOPs, 0.005% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.002 GFLOPs, 0.002% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            0.067 M, 0.269% Params, 0.835 GFLOPs, 1.266% FLOPs, 
            (conv): Conv2d(0.066 M, 0.264% Params, 0.822 GFLOPs, 1.247% FLOPs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.004% Params, 0.013 GFLOPs, 0.019% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.006 GFLOPs, 0.010% FLOPs, inplace=True)
        )
        (tam): TAM(
          0.017 M, 0.067% Params, 0.0 GFLOPs, 0.000% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.016 M, 0.066% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Conv1d(0.012 M, 0.050% Params, 0.0 GFLOPs, 0.000% FLOPs, 128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.004 M, 0.017% Params, 0.0 GFLOPs, 0.000% FLOPs, 32, 128, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
      (2): TABlock(
        0.297 M, 1.197% Params, 3.523 GFLOPs, 5.343% FLOPs, 
        (block): Bottleneck(
          0.28 M, 1.130% Params, 3.523 GFLOPs, 5.342% FLOPs, 
          (conv1): ConvModule(
            0.066 M, 0.265% Params, 0.827 GFLOPs, 1.254% FLOPs, 
            (conv): Conv2d(0.066 M, 0.264% Params, 0.822 GFLOPs, 1.247% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.003 GFLOPs, 0.005% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.002 GFLOPs, 0.002% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            0.148 M, 0.596% Params, 1.855 GFLOPs, 2.812% FLOPs, 
            (conv): Conv2d(0.147 M, 0.595% Params, 1.85 GFLOPs, 2.805% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.003 GFLOPs, 0.005% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.002 GFLOPs, 0.002% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            0.067 M, 0.269% Params, 0.835 GFLOPs, 1.266% FLOPs, 
            (conv): Conv2d(0.066 M, 0.264% Params, 0.822 GFLOPs, 1.247% FLOPs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.004% Params, 0.013 GFLOPs, 0.019% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.006 GFLOPs, 0.010% FLOPs, inplace=True)
        )
        (tam): TAM(
          0.017 M, 0.067% Params, 0.0 GFLOPs, 0.000% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.016 M, 0.066% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Conv1d(0.012 M, 0.050% Params, 0.0 GFLOPs, 0.000% FLOPs, 128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.004 M, 0.017% Params, 0.0 GFLOPs, 0.000% FLOPs, 32, 128, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
      (3): TABlock(
        0.297 M, 1.197% Params, 3.523 GFLOPs, 5.343% FLOPs, 
        (block): Bottleneck(
          0.28 M, 1.130% Params, 3.523 GFLOPs, 5.342% FLOPs, 
          (conv1): ConvModule(
            0.066 M, 0.265% Params, 0.827 GFLOPs, 1.254% FLOPs, 
            (conv): Conv2d(0.066 M, 0.264% Params, 0.822 GFLOPs, 1.247% FLOPs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.003 GFLOPs, 0.005% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.002 GFLOPs, 0.002% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            0.148 M, 0.596% Params, 1.855 GFLOPs, 2.812% FLOPs, 
            (conv): Conv2d(0.147 M, 0.595% Params, 1.85 GFLOPs, 2.805% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.0 M, 0.001% Params, 0.003 GFLOPs, 0.005% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.002 GFLOPs, 0.002% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            0.067 M, 0.269% Params, 0.835 GFLOPs, 1.266% FLOPs, 
            (conv): Conv2d(0.066 M, 0.264% Params, 0.822 GFLOPs, 1.247% FLOPs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.004% Params, 0.013 GFLOPs, 0.019% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.006 GFLOPs, 0.010% FLOPs, inplace=True)
        )
        (tam): TAM(
          0.017 M, 0.067% Params, 0.0 GFLOPs, 0.000% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.016 M, 0.066% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Conv1d(0.012 M, 0.050% Params, 0.0 GFLOPs, 0.000% FLOPs, 128, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.004 M, 0.017% Params, 0.0 GFLOPs, 0.000% FLOPs, 32, 128, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
    )
    (layer3): Sequential(
      7.493 M, 30.236% Params, 23.537 GFLOPs, 35.694% FLOPs, 
      (0): TABlock(
        1.578 M, 6.369% Params, 5.989 GFLOPs, 9.083% FLOPs, 
        (block): Bottleneck(
          1.512 M, 6.103% Params, 5.988 GFLOPs, 9.081% FLOPs, 
          (conv1): ConvModule(
            0.132 M, 0.531% Params, 1.654 GFLOPs, 2.508% FLOPs, 
            (conv): Conv2d(0.131 M, 0.529% Params, 1.644 GFLOPs, 2.493% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.006 GFLOPs, 0.010% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.003 GFLOPs, 0.005% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            0.59 M, 2.382% Params, 1.852 GFLOPs, 2.809% FLOPs, 
            (conv): Conv2d(0.59 M, 2.380% Params, 1.85 GFLOPs, 2.805% FLOPs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GFLOPs, 0.002% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.001 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            0.264 M, 1.066% Params, 0.829 GFLOPs, 1.256% FLOPs, 
            (conv): Conv2d(0.262 M, 1.058% Params, 0.822 GFLOPs, 1.247% FLOPs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.002 M, 0.008% Params, 0.006 GFLOPs, 0.010% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.003 GFLOPs, 0.005% FLOPs, inplace=True)
          (downsample): ConvModule(
            0.526 M, 2.124% Params, 1.651 GFLOPs, 2.503% FLOPs, 
            (conv): Conv2d(0.524 M, 2.116% Params, 1.644 GFLOPs, 2.493% FLOPs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (bn): BatchNorm2d(0.002 M, 0.008% Params, 0.006 GFLOPs, 0.010% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (tam): TAM(
          0.066 M, 0.265% Params, 0.001 GFLOPs, 0.002% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.066 M, 0.265% Params, 0.001 GFLOPs, 0.002% FLOPs, 
            (0): Conv1d(0.049 M, 0.198% Params, 0.001 GFLOPs, 0.001% FLOPs, 256, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.001% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.016 M, 0.066% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, 256, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
      (1): TABlock(
        1.183 M, 4.774% Params, 3.509 GFLOPs, 5.322% FLOPs, 
        (block): Bottleneck(
          1.117 M, 4.508% Params, 3.508 GFLOPs, 5.320% FLOPs, 
          (conv1): ConvModule(
            0.263 M, 1.060% Params, 0.824 GFLOPs, 1.250% FLOPs, 
            (conv): Conv2d(0.262 M, 1.058% Params, 0.822 GFLOPs, 1.247% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GFLOPs, 0.002% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.001 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            0.59 M, 2.382% Params, 1.852 GFLOPs, 2.809% FLOPs, 
            (conv): Conv2d(0.59 M, 2.380% Params, 1.85 GFLOPs, 2.805% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GFLOPs, 0.002% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.001 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            0.264 M, 1.066% Params, 0.829 GFLOPs, 1.256% FLOPs, 
            (conv): Conv2d(0.262 M, 1.058% Params, 0.822 GFLOPs, 1.247% FLOPs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.002 M, 0.008% Params, 0.006 GFLOPs, 0.010% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.003 GFLOPs, 0.005% FLOPs, inplace=True)
        )
        (tam): TAM(
          0.066 M, 0.265% Params, 0.001 GFLOPs, 0.002% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.066 M, 0.265% Params, 0.001 GFLOPs, 0.002% FLOPs, 
            (0): Conv1d(0.049 M, 0.198% Params, 0.001 GFLOPs, 0.001% FLOPs, 256, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.001% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.016 M, 0.066% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, 256, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
      (2): TABlock(
        1.183 M, 4.774% Params, 3.509 GFLOPs, 5.322% FLOPs, 
        (block): Bottleneck(
          1.117 M, 4.508% Params, 3.508 GFLOPs, 5.320% FLOPs, 
          (conv1): ConvModule(
            0.263 M, 1.060% Params, 0.824 GFLOPs, 1.250% FLOPs, 
            (conv): Conv2d(0.262 M, 1.058% Params, 0.822 GFLOPs, 1.247% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GFLOPs, 0.002% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.001 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            0.59 M, 2.382% Params, 1.852 GFLOPs, 2.809% FLOPs, 
            (conv): Conv2d(0.59 M, 2.380% Params, 1.85 GFLOPs, 2.805% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GFLOPs, 0.002% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.001 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            0.264 M, 1.066% Params, 0.829 GFLOPs, 1.256% FLOPs, 
            (conv): Conv2d(0.262 M, 1.058% Params, 0.822 GFLOPs, 1.247% FLOPs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.002 M, 0.008% Params, 0.006 GFLOPs, 0.010% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.003 GFLOPs, 0.005% FLOPs, inplace=True)
        )
        (tam): TAM(
          0.066 M, 0.265% Params, 0.001 GFLOPs, 0.002% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.066 M, 0.265% Params, 0.001 GFLOPs, 0.002% FLOPs, 
            (0): Conv1d(0.049 M, 0.198% Params, 0.001 GFLOPs, 0.001% FLOPs, 256, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.001% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.016 M, 0.066% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, 256, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
      (3): TABlock(
        1.183 M, 4.774% Params, 3.509 GFLOPs, 5.322% FLOPs, 
        (block): Bottleneck(
          1.117 M, 4.508% Params, 3.508 GFLOPs, 5.320% FLOPs, 
          (conv1): ConvModule(
            0.263 M, 1.060% Params, 0.824 GFLOPs, 1.250% FLOPs, 
            (conv): Conv2d(0.262 M, 1.058% Params, 0.822 GFLOPs, 1.247% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GFLOPs, 0.002% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.001 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            0.59 M, 2.382% Params, 1.852 GFLOPs, 2.809% FLOPs, 
            (conv): Conv2d(0.59 M, 2.380% Params, 1.85 GFLOPs, 2.805% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GFLOPs, 0.002% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.001 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            0.264 M, 1.066% Params, 0.829 GFLOPs, 1.256% FLOPs, 
            (conv): Conv2d(0.262 M, 1.058% Params, 0.822 GFLOPs, 1.247% FLOPs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.002 M, 0.008% Params, 0.006 GFLOPs, 0.010% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.003 GFLOPs, 0.005% FLOPs, inplace=True)
        )
        (tam): TAM(
          0.066 M, 0.265% Params, 0.001 GFLOPs, 0.002% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.066 M, 0.265% Params, 0.001 GFLOPs, 0.002% FLOPs, 
            (0): Conv1d(0.049 M, 0.198% Params, 0.001 GFLOPs, 0.001% FLOPs, 256, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.001% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.016 M, 0.066% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, 256, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
      (4): TABlock(
        1.183 M, 4.774% Params, 3.509 GFLOPs, 5.322% FLOPs, 
        (block): Bottleneck(
          1.117 M, 4.508% Params, 3.508 GFLOPs, 5.320% FLOPs, 
          (conv1): ConvModule(
            0.263 M, 1.060% Params, 0.824 GFLOPs, 1.250% FLOPs, 
            (conv): Conv2d(0.262 M, 1.058% Params, 0.822 GFLOPs, 1.247% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GFLOPs, 0.002% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.001 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            0.59 M, 2.382% Params, 1.852 GFLOPs, 2.809% FLOPs, 
            (conv): Conv2d(0.59 M, 2.380% Params, 1.85 GFLOPs, 2.805% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GFLOPs, 0.002% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.001 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            0.264 M, 1.066% Params, 0.829 GFLOPs, 1.256% FLOPs, 
            (conv): Conv2d(0.262 M, 1.058% Params, 0.822 GFLOPs, 1.247% FLOPs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.002 M, 0.008% Params, 0.006 GFLOPs, 0.010% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.003 GFLOPs, 0.005% FLOPs, inplace=True)
        )
        (tam): TAM(
          0.066 M, 0.265% Params, 0.001 GFLOPs, 0.002% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.066 M, 0.265% Params, 0.001 GFLOPs, 0.002% FLOPs, 
            (0): Conv1d(0.049 M, 0.198% Params, 0.001 GFLOPs, 0.001% FLOPs, 256, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.001% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.016 M, 0.066% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, 256, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
      (5): TABlock(
        1.183 M, 4.774% Params, 3.509 GFLOPs, 5.322% FLOPs, 
        (block): Bottleneck(
          1.117 M, 4.508% Params, 3.508 GFLOPs, 5.320% FLOPs, 
          (conv1): ConvModule(
            0.263 M, 1.060% Params, 0.824 GFLOPs, 1.250% FLOPs, 
            (conv): Conv2d(0.262 M, 1.058% Params, 0.822 GFLOPs, 1.247% FLOPs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GFLOPs, 0.002% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.001 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            0.59 M, 2.382% Params, 1.852 GFLOPs, 2.809% FLOPs, 
            (conv): Conv2d(0.59 M, 2.380% Params, 1.85 GFLOPs, 2.805% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GFLOPs, 0.002% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.001 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            0.264 M, 1.066% Params, 0.829 GFLOPs, 1.256% FLOPs, 
            (conv): Conv2d(0.262 M, 1.058% Params, 0.822 GFLOPs, 1.247% FLOPs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.002 M, 0.008% Params, 0.006 GFLOPs, 0.010% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.003 GFLOPs, 0.005% FLOPs, inplace=True)
        )
        (tam): TAM(
          0.066 M, 0.265% Params, 0.001 GFLOPs, 0.002% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.066 M, 0.265% Params, 0.001 GFLOPs, 0.002% FLOPs, 
            (0): Conv1d(0.049 M, 0.198% Params, 0.001 GFLOPs, 0.001% FLOPs, 256, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.001% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.016 M, 0.066% Params, 0.0 GFLOPs, 0.000% FLOPs, 64, 256, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
    )
    (layer4): Sequential(
      15.752 M, 63.566% Params, 12.989 GFLOPs, 19.699% FLOPs, 
      (0): TABlock(
        6.302 M, 25.431% Params, 5.979 GFLOPs, 9.067% FLOPs, 
        (block): Bottleneck(
          6.04 M, 24.372% Params, 5.974 GFLOPs, 9.060% FLOPs, 
          (conv1): ConvModule(
            0.525 M, 2.120% Params, 1.649 GFLOPs, 2.501% FLOPs, 
            (conv): Conv2d(0.524 M, 2.116% Params, 1.644 GFLOPs, 2.493% FLOPs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.004% Params, 0.003 GFLOPs, 0.005% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.002 GFLOPs, 0.002% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            2.36 M, 9.525% Params, 1.851 GFLOPs, 2.807% FLOPs, 
            (conv): Conv2d(2.359 M, 9.521% Params, 1.85 GFLOPs, 2.805% FLOPs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.004% Params, 0.001 GFLOPs, 0.001% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            1.053 M, 4.248% Params, 0.825 GFLOPs, 1.252% FLOPs, 
            (conv): Conv2d(1.049 M, 4.231% Params, 0.822 GFLOPs, 1.247% FLOPs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.004 M, 0.017% Params, 0.003 GFLOPs, 0.005% FLOPs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.002 GFLOPs, 0.002% FLOPs, inplace=True)
          (downsample): ConvModule(
            2.101 M, 8.479% Params, 1.647 GFLOPs, 2.498% FLOPs, 
            (conv): Conv2d(2.097 M, 8.463% Params, 1.644 GFLOPs, 2.493% FLOPs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (bn): BatchNorm2d(0.004 M, 0.017% Params, 0.003 GFLOPs, 0.005% FLOPs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (tam): TAM(
          0.262 M, 1.059% Params, 0.004 GFLOPs, 0.007% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.262 M, 1.059% Params, 0.004 GFLOPs, 0.006% FLOPs, 
            (0): Conv1d(0.197 M, 0.793% Params, 0.003 GFLOPs, 0.005% FLOPs, 512, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.001% Params, 0.0 GFLOPs, 0.000% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.066 M, 0.264% Params, 0.001 GFLOPs, 0.002% FLOPs, 128, 512, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
      (1): TABlock(
        4.725 M, 19.067% Params, 3.505 GFLOPs, 5.316% FLOPs, 
        (block): Bottleneck(
          4.463 M, 18.008% Params, 3.501 GFLOPs, 5.310% FLOPs, 
          (conv1): ConvModule(
            1.05 M, 4.236% Params, 0.823 GFLOPs, 1.249% FLOPs, 
            (conv): Conv2d(1.049 M, 4.231% Params, 0.822 GFLOPs, 1.247% FLOPs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.004% Params, 0.001 GFLOPs, 0.001% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            2.36 M, 9.525% Params, 1.851 GFLOPs, 2.807% FLOPs, 
            (conv): Conv2d(2.359 M, 9.521% Params, 1.85 GFLOPs, 2.805% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.004% Params, 0.001 GFLOPs, 0.001% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            1.053 M, 4.248% Params, 0.825 GFLOPs, 1.252% FLOPs, 
            (conv): Conv2d(1.049 M, 4.231% Params, 0.822 GFLOPs, 1.247% FLOPs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.004 M, 0.017% Params, 0.003 GFLOPs, 0.005% FLOPs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.002 GFLOPs, 0.002% FLOPs, inplace=True)
        )
        (tam): TAM(
          0.262 M, 1.059% Params, 0.004 GFLOPs, 0.007% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.262 M, 1.059% Params, 0.004 GFLOPs, 0.006% FLOPs, 
            (0): Conv1d(0.197 M, 0.793% Params, 0.003 GFLOPs, 0.005% FLOPs, 512, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.001% Params, 0.0 GFLOPs, 0.000% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.066 M, 0.264% Params, 0.001 GFLOPs, 0.002% FLOPs, 128, 512, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
      (2): TABlock(
        4.725 M, 19.067% Params, 3.505 GFLOPs, 5.316% FLOPs, 
        (block): Bottleneck(
          4.463 M, 18.008% Params, 3.501 GFLOPs, 5.310% FLOPs, 
          (conv1): ConvModule(
            1.05 M, 4.236% Params, 0.823 GFLOPs, 1.249% FLOPs, 
            (conv): Conv2d(1.049 M, 4.231% Params, 0.822 GFLOPs, 1.247% FLOPs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.004% Params, 0.001 GFLOPs, 0.001% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv2): ConvModule(
            2.36 M, 9.525% Params, 1.851 GFLOPs, 2.807% FLOPs, 
            (conv): Conv2d(2.359 M, 9.521% Params, 1.85 GFLOPs, 2.805% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(0.001 M, 0.004% Params, 0.001 GFLOPs, 0.001% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.001% FLOPs, inplace=True)
          )
          (conv3): ConvModule(
            1.053 M, 4.248% Params, 0.825 GFLOPs, 1.252% FLOPs, 
            (conv): Conv2d(1.049 M, 4.231% Params, 0.822 GFLOPs, 1.247% FLOPs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(0.004 M, 0.017% Params, 0.003 GFLOPs, 0.005% FLOPs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (relu): ReLU(0.0 M, 0.000% Params, 0.002 GFLOPs, 0.002% FLOPs, inplace=True)
        )
        (tam): TAM(
          0.262 M, 1.059% Params, 0.004 GFLOPs, 0.007% FLOPs, 
          (G): Sequential(
            0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 
            (0): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=4, out_features=8, bias=False)
            (1): BatchNorm1d(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Linear(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=8, out_features=3, bias=False)
            (4): Softmax(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, dim=-1)
          )
          (L): Sequential(
            0.262 M, 1.059% Params, 0.004 GFLOPs, 0.006% FLOPs, 
            (0): Conv1d(0.197 M, 0.793% Params, 0.003 GFLOPs, 0.005% FLOPs, 512, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (1): BatchNorm1d(0.0 M, 0.001% Params, 0.0 GFLOPs, 0.000% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, inplace=True)
            (3): Conv1d(0.066 M, 0.264% Params, 0.001 GFLOPs, 0.002% FLOPs, 128, 512, kernel_size=(1,), stride=(1,), bias=False)
            (4): Sigmoid(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
          )
        )
      )
    )
  )
  (cls_head): TSMHead(
    0.012 M, 0.050% Params, 0.002 GFLOPs, 0.003% FLOPs, 
    (loss_cls): CrossEntropyLoss(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
    (consensus): AvgConsensus(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, )
    (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GFLOPs, 0.000% FLOPs, p=0.5, inplace=False)
    (fc_cls): Linear(0.012 M, 0.050% Params, 0.0 GFLOPs, 0.000% FLOPs, in_features=2048, out_features=6, bias=True)
    (avg_pool): AdaptiveAvgPool2d(0.0 M, 0.000% Params, 0.002 GFLOPs, 0.002% FLOPs, output_size=1)
  )
)
==============================
Input shape: (16, 3, 224, 224)
Flops: 65.94 GFLOPs
Params: 24.78 M
==============================

